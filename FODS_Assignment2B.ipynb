{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hitters.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 59 samples don't have salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Strings to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['League'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Division'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NewLeague'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_columns(column):\n",
    "    unique_values = df[column].unique()\n",
    "    mapping = {value: index + 1 for index, value in enumerate(unique_values)}\n",
    "    df[column] = df[column].map(mapping)\n",
    "    print(f\"Mapping for {column}: {mapping}\")\n",
    "\n",
    "columns_to_map = ['League', 'Division', 'NewLeague']\n",
    "for column in columns_to_map:\n",
    "    map_columns(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, features = np.shape(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Salary'])\n",
    "y = df['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "X_standardized = (X - X.mean()) / X.std()\n",
    "\n",
    "df_pca = X_standardized\n",
    "df_pca['Salary'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the covariance matrix\n",
    "covariance_matrix = np.cov(X_standardized, rowvar=False)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors in descending order\n",
    "eig_pairs = [(eigenvalues[i], eigenvectors[:, i]) for i in range(len(eigenvalues))]\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of components for efficient prediction\n",
    "total_variance = sum(eigenvalues)\n",
    "explained_variance = [eigenvalue / total_variance for eigenvalue in eigenvalues]\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Find the number of components that explain at least 90% of the variance (only for representation purposes)\n",
    "desired_explained_variance = 0.90\n",
    "num_components = np.argmax(cumulative_explained_variance >= desired_explained_variance) + 1\n",
    "num_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variance to visualize the relationship\n",
    "plt.plot(range(1, len(eigenvalues) + 1), cumulative_explained_variance, marker='o')\n",
    "plt.axvline(x=num_components, color='r', linestyle='--', label=f'{desired_explained_variance * 100}% Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Number of Components vs Cumulative Explained Variance')\n",
    "plt.savefig('2B1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "train_fraction = 0.8\n",
    "train = df_pca.sample(frac=train_fraction, random_state=seed)\n",
    "test = df_pca.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Salary' is the column you want to predict\n",
    "X_train = train.drop('Salary', axis=1)  # Features for training\n",
    "y_train = train['Salary']  # Target for training\n",
    "\n",
    "X_test = test.drop('Salary', axis=1)  # Features for testing\n",
    "y_test = test['Salary']  # Target for testing\n",
    "\n",
    "# Convert labels to numpy array for applying ML Models\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(x, y, lr=0.00001, e=0.9):\n",
    "    # Insert new column with ones (bias)\n",
    "    regression = np.c_[x, np.ones(len(x))]\n",
    "    # Weights with same width as x\n",
    "    weights = np.ones(regression.shape[1])\n",
    "    # Gradient Descent\n",
    "    norma = 1\n",
    "    while(norma > e):\n",
    "        y_pred = regression @ weights\n",
    "        partial = regression.T @ (y-y_pred)\n",
    "        norma = np.sum(np.sqrt(np.square(partial)))\n",
    "\n",
    "        weights = weights.T + (lr*partial)\n",
    "\n",
    "        if np.isnan(norma):\n",
    "            print('MODEL DIVERGED! USE LOWER LEARNING RATE!')\n",
    "        \n",
    "    return weights\n",
    "\n",
    "def predict(w, x):\n",
    "    return w[:-1] @ np.array(x).T + w[-1]\n",
    "\n",
    "def MSE(y, y_pred):\n",
    "    return np.sum(np.square(y - y_pred))/float(len(y))\n",
    "\n",
    "def MAE(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fit a linear regression model and calculate MSE\n",
    "def fit_and_evaluate_pca_regression(X_train, y_train, X_test, y_test, num_components):\n",
    "    # Project the original data onto the selected number of components\n",
    "    selected_eigenvectors = eigenvectors[:, :num_components]\n",
    "    X_train_pca = np.dot(X_train, selected_eigenvectors)\n",
    "    X_test_pca = np.dot(X_test, selected_eigenvectors)\n",
    "\n",
    "    # Fit linear regression using gradient descent\n",
    "    weights = fit_linear_regression(X_train_pca, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = predict(weights, X_test_pca)\n",
    "\n",
    "    # Calculate MSE\n",
    "    rmse = np.sqrt(MSE(y_test, y_pred))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a range of principal component numbers\n",
    "component_numbers = range(1, len(eigenvalues) + 1)\n",
    "\n",
    "# Store MSE values for each number of components\n",
    "rmse_values = []\n",
    "\n",
    "# Iterate over component numbers\n",
    "for num_components in component_numbers:\n",
    "    rmse = fit_and_evaluate_pca_regression(X_train.to_numpy(), y_train, X_test.to_numpy(), y_test, num_components)\n",
    "    rmse_values.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plotting Number of Components vs RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the RMSE values for different numbers of components\n",
    "plt.plot(component_numbers, rmse_values, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Root Mean Squared Error (RMSE)')\n",
    "plt.title('Number of Components vs RMSE')\n",
    "\n",
    "# Identify the Stable RMSE and plot a line\n",
    "min_rmse_index = 6\n",
    "min_rmse = rmse_values[min_rmse_index]\n",
    "plt.axvline(x=min_rmse_index + 1, color='r', linestyle='--', label=f'Stable RMSE: {min_rmse:.2f} (at {min_rmse_index + 1} components)')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('2B2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Most Efficient Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_num_components = min_rmse_index + 1\n",
    "\n",
    "# Project the original data onto the selected optimal number of components\n",
    "selected_eigenvectors = eigenvectors[:, :optimal_num_components]\n",
    "X_train_optimal_pca = np.dot(X_train.to_numpy(), selected_eigenvectors)\n",
    "X_test_optimal_pca = np.dot(X_test.to_numpy(), selected_eigenvectors)\n",
    "\n",
    "# Fit linear regression using gradient descent\n",
    "weights_optimal = fit_linear_regression(X_train_optimal_pca, y_train)\n",
    "\n",
    "# Choose a specific point for prediction\n",
    "specific_point = X_test_optimal_pca[0]\n",
    "\n",
    "# Make a prediction for the specific point using the selected model\n",
    "y_pred = predict(weights_optimal, specific_point)\n",
    "\n",
    "# Print the predicted y value\n",
    "print(\"Predicted y value:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the Graph:**\n",
    "The graph of the number of components vs RMSE provides valuable insights into the trade-off between model complexity and prediction accuracy. In the plot, we observed how the RMSE changes as the number of principal components increases. The key point of interest is where the RMSE reaches a minimum or starts stabilizing. This stable point represents the optimal number of components for building an efficient predictive model. In the plot, we identified this point and marked it with a red dashed line.\n",
    "\n",
    "**Significance of Selecting an Appropriate Number of Components:**\n",
    "Selecting an appropriate number of components is crucial for achieving a balance between model simplicity and predictive accuracy. Too few components may lead to underfitting, where the model fails to capture important patterns in the data. On the other hand, too many components can result in overfitting, where the model fits the training data too closely and fails to generalize well to new, unseen data.\n",
    "\n",
    "The significance lies in finding the sweet spot where the model captures the essential information in the data while avoiding unnecessary complexity. The optimal number of components identified from the graph represents the model configuration that strikes this balance, offering a good compromise between accuracy and efficiency.\n",
    "\n",
    "**Analysis of the Predicted Value (y_pred):**\n",
    "After selecting the optimal model based on the number of components, we tested its performance by predicting a specific data point (y_pred). The predicted value (y_pred) represents the model's estimate of the target variable for that particular input. It is essential to analyze the significance of this prediction in the context of your specific application.\n",
    "\n",
    "**Accuracy Assessment:**\n",
    "We can compare the predicted value (y_pred) with the actual target value to assess the accuracy of the model. we have calculated Mean Absolute Error (MAE), to provide a more comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = MAE(y_test, np.array([predict(weights_optimal, point) for point in X_test_optimal_pca]))\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
